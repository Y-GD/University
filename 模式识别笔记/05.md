# 第五章 特征选择与特征提取



## 一、基本概念



### 1、特征选择与特征提取的定义



#### 特征选择

所谓特征选择，就是从$n$个度量值的集合$\{x_1,x_2,\dots,x_n\}$中，按一准则选取$m$维($m<n$)供分类用的子集，作为降维的分类特征



#### 特征提取

所谓特征提取，就是使$(x_1,x_2,\dots,x_n)$通过某种变换，产生$m$个特征$(y_1,y_2,\dots,y_m)(m<n)$，作为新的分类特征(或称为二次特征)



#### Notice

1. 特征选择的关键在于选择有价值的特征
2. 特征提取的关键在于变换，以获得更重要的特征





### 2、特征选择与特征提取的必要性

- **去除不必要的特征**

  在很多实际问题中，往往不容易找到那些最重要的特征，或受客观条件的限制，不能对它们进行有效的测量，因此，应去掉模棱两可、不易判别的特征。

- **凸显重要特征**

  由于客观上的需要，为了突出某些有用信息，抑制无用信息，有意加上一些比值、指数或对数等组合计算特征。

- **特征维数灾难**

  如果将数目很多的测量值不做分析，全部直接用作分类特征，不但耗时，而且会影响到分类的效果，产生“特征维数灾难”问题。

- **提高分类效率**

  在尽可能保留识别信息的前提下，降低特征空间的维数，以达到有效的分类。

- **总体目标**

  使用有价值的特征，使分类器实现快速、准确、高效的分类



### 3、特征选择和特征提取的常用方法



#### 直接选择法

当实际用于分类识别的特征数目$d$确定后，直接从已获得的$n$个原始特征中选取$d$个特征$x_1,x_2,\dots,x_d$，使得可分性判据$J$满足：
$$
J(x_1,x_2,\dots,x_d)=\max{[J(x_{i1},x_{i2},\dots,x_{id})]}
$$
$x_{i1},x_{i2},\dots,x_{id}$是$n$个原始特征中的任意$d$个特征，上式表示直接寻找$n$维特征空间中的$d$维子空间



**常用方法**

- 分支定界法
- 按回归建模技术确定相关特征



#### 变换法

在使判据$J$取最大的目标下，对$n$个原始特征进行变换降维，即对原$n$维特征空间进行**坐标变换**，然后取子空间



**常用方法**

- 基于可分性判据的特征选择
- 基于误判概率的特征选择
- 离散$K-L$变换法(DKLT)
- 基于决策界的特征选择



## 二、类别可分性测度



### 1、基本概念



#### 类别可分性测度的目标

为确立特征提取和选择的准则，引入类别可分性判据，来**刻画特征对分类的贡献**



#### 类别可分性测度的要求

- 单调性

  测度与误判概率(或误分概率的上界、下界)有单调关系

- 可加性。当特征相互独立时，判据有可加性，即：

  $J_{ij}(x_1,x_2,\dots,x_d)=\sum\limits_{k=1}^dJ_{ij}(x_k)$

  式中$x_1,x_2,\dots,x_d$是对不同种类特征的测量值$J_{ij}(\cdot)$表示使用括号中特征时第$i$类与第$j$类可分性判据函数

- 距离性。判据具有距离的某些特征，即：

  $当i\ne j时，J_{ij}>0\\当 i=j时，J_{ij}=0\\J_{ij}=J_{ji}$

- 单调不减性。对特征数目是单调不减的，即加入新的特征后，判据值不变：

  $J_{ij}(x_1,x_2,\dots,x_d)\le J_{ij}(x_1,x_2,\dots,x_d,x_{d+1})$



### 2、基于距离的可分性测度



#### 原理

一般来讲，不同类的模式可以被区分是由于他们所属类别在特征空间中的类域是不同的区域。可以用距离或离差测度(散度)来构造类别的可分性判据



#### 原则

- 区域重叠的部分越小或完全没有重叠，类别可分性越好
- 当类内模式较密聚，而不同类的模式相聚较远时，类别可分性好



#### 类内距离指标



**类内距离和类内散布矩阵**

- 类内距离

  类内各样本间的均方距离简称类内距离
  $$
  d^2(w_i)=\frac1{N_{i}}\sum\limits_{k=1}^{N_i}(x_k^{(i)}-m^{(i)})^T(x_k^{(i)}-m^{(i)})
  $$
  或者

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191129153836717.png" alt="image-20191129153836717" style="zoom: 80%;" />

- 类内散布矩阵

  该类的协方差矩阵



**均方距离与类内散布矩阵的关系**

- 均方距离是类内散布矩阵的迹（对角线上元素之和）的平均值
- 迹是每个特征分量的方差之和



**可分性测度讨论**

类内距离越小越好，越利于特征提取和分类



#### 类间距离指标



**多类模式的类间距**

每一类均值向量与模式总体均值向量之间平方距离的先验概率加权和
$$
D_b^2=\sum\limits_{i=1}^c p(w_i)[M_i-M_0]^2
$$
其中，$c为类别数，M_i为w_i类的均值，M_0为所有样本的均值$



**散布矩阵**
$$
S_b=\sum\limits_{i=1}^c p(w_i)(M_i-M_0)(M_i-M_0)^T
$$

**类间距离与类间散布矩阵的关系**

- 类间距离是类间散布矩阵的迹（对角线上元素之和）
- 迹是每个特征分量的方差之和



**可分性测度**

类间距离越大越好，越利于特征提取和分类



#### 总体距离指标



**多类模式向量间的距离和总体散布矩阵**

- 类间散布矩阵

  $S_b=\sum\limits_{i=1}^{c}p(w_i)(M_i-M_0)(M_i-M_0)^T$

- 多类类内散布矩阵：各类模式协方差矩阵的先验概率加权平均值

  $S_w=\sum\limits_{i=1}^cp(w_i) \frac1{n_i}\sum\limits_{k=1}^{n_i}(X_k^i-M_i)(X_k^i-M_i)^T$

- 总体散布矩阵：各类类内散布矩阵与类间散布矩阵之和

  $S_t=S_b+S_w$



**可分性测度讨论**

特征选择与特征提取应该是类间距离($tr(S_b)$)尽量大，类内$tr(S_w)$距离尽量小



**可分性判据**

可以用$S_t,S_b,S_w$构造不同的可分性判据：
$$
\begin{align}
&J_1=Tr[S_w^{-1}S_b]\\
&J_2=ln[|\frac{S_b}{S_w}|]\\
&J_3=\frac{Tr[S_b]}{Tr[S_w]}\\
&J_4=\frac{|S_w+S_b|}{|S_w|}=\frac{|S_t|}{|S_w|}
\end{align}
$$
数字越大越好





### 3、基于概率分布的可分性测度



#### 基本概念

- **基本思路**

  使用两类概率密度函数的重叠程度来度量可分性，作为可分性判据

- 可分性判据应该满足：

  1. $J_p\ge0$
  2. 当量类概率密度函数完全不重叠时，$J_p=max$
  3. 当两类概率密度函数完全重合时，$J_p=0$
  4. 相对两个概率具有"对称性"(如：判据呈倒数关系)

- 常用可分性测度方法：
  1. 散度$J_D$
  2. $Bhattacharyya$判据$J_B$
  3. $Chernoff$判据$J_C$

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191129160359928.png" alt="image-20191129160359928" style="zoom:67%;" />



#### 散度



**平均可分性信息的定义**

- $w_i$类对$w_j$类的平均可分性信息为其<u>对数似然比的期望值</u>
  $$
  I_{ij}=E[ln\frac{P(X|w_i)}{P(X|w_j)}]=\int_XP(X|w_i)ln\frac{P(X|w_i)}{P(X|w_j)}dX
  $$

- $w_j$类对$w_i$类的平均可分性信息为其<u>对数似然比的期望值</u>：
  $$
  I_{ji}=E[ln\frac{P(X|w_j)}{P(X|w_i)}]=\int_XP(X|w_j)ln\frac{P(X|w_j)}{P(X|w_i)}dX
  $$
  



**散度**

- 定义

  两类平均可分性信息之和

- 数学表达式
  $$
  \begin{align}
  J_D&=J_{ij}\\&=I_i+I_j\\&=\int_X(p(X|w_i)-P(X|w_j))ln\frac{P(X|w_i)}{P(X|w_j)}dX
  \end{align}
  $$

- 性质
  1. $J_{ij}\ge0$
  
  2. 对称性
     $$
     J_{ij}(w_1,w_2)=J_{ij}(w_2,w_1)
     $$
  
  3. $I_{ij}=0\Leftrightarrow P(X|w_i)=P(X|w_j)$
  
  4. 可加性。
  
     对于模式向量$X=(x_1,x_2,\dots,x_n)^T$，若各分量相互独立，则：
     $$
     J_{ij}(X)=J_{ij}(x_1,x_2,\dots,x_n)=\sum\limits_{k=1}^nJ_{ij}(x_k)
     $$
  
  5. 对新特征的可加性
  
     对于模式向量$X=(x_1,x_2,\dots,x_n)^T$，若各分量相互独立，则$J_{ij}(x_1,x_2,\dots,x_n)\le J_{ij}(x_1,x_2,\dots,x_n,x_{n+1})$



**两个正态分布模式类的散度**

- 当两类都是正态分布时：
  $$
  \begin{align}
  J_{ij}&=I_{ij}+I_{ji}\\
  &=\frac12 tr[(C_j^{-1}-C_i^{-1})(C_i-C_j)]+\frac12tr[(C_i^{-1}+C_j^{-1})(M_i-M_j)(M_i-M_j)^T]
  \end{align}
  $$

- 当两类协方差矩阵相等时，即$C_i=C_j=C$时：
  $$
  \begin{align}
  J_{ij}&=tr[C^{-1}(M_i-M_j)(M_i-M_j)^T]\\
  &=tr[(M_i-M_j)^TC^{-1}(M_i-M_j)]
  \end{align}
  $$
  即散度为马氏距离的平方

- 对于一维正态分布，有：
  $$
  J_{ij}=\frac{(m_i-m_j)^2}{\sigma^2}
  $$
  均值向量距离越远，方差越小，散度越大





#### Bhattacharyya判据

定义：
$$
J_B=-ln\int_x\sqrt{P(X|w_1)P(X|w_2)}dX
$$
在正态分布下，且$C_1=C_2=C$时，有：
$$
J_B=\frac18(M_1-M_2)^TC^{-1}(M_1-M_2)=\frac18J_D
$$
**Bhattacharyya判据也是马氏距离的平方，只是系数不同**



#### Chernoff判据

定义：
$$
J_C=u(s)=-ln\int_XP(X|w_1)^{1-s}p(X|w_2)^sdX
$$
其中，$s$为$[0,1]$区间的一个参数



**Chernoff判据是Bhattacharyya判据的一般形式，当s=1/2时， Chernoff判据就是Bhattacharyya判据**



#### 多类情况下的推广

- 多类情况下的散度
  $$
  J_D=\sum\limits_{i=1}^c\sum\limits_{j=i+1}^cP(w_i)P(w_j)J_D(w_i,w_j)
  $$

- 多类情况下的Bhattacharyya判据
  $$
  J_B=\sum\limits_{i=1}^c\sum\limits_{j=i+1}^cP(w_i)P(w_j)J_B(w_i,w_j)
  $$
  
- 多类情况下的Chernoff判据
  $$
  J_C=\sum\limits_{i=1}^c\sum\limits_{j=i+1}^cP(w_i)P(w_j)J_C(w_i,w_j)
  $$



#### 判据与错误率的关系

- **两类问题的最小错误率**

  当两类先验概率相等，且协方差相同时，正态分布的判决线为两者均值的中间，此时错误率较小

- **散度$J_D$与最小错误率的关系**

  1. 散度$J_D$为均值之间的距离，决定判决线的位置
  2. 错误率为判决线右边阴影部分的面积

- **总体关系**

  判据值越大，错误率上界越小

![image-20191129195602066](C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191129195602066.png)



#### 基于后验概率的可分性测度

- 熵越大越不确定，故可以用熵来衡量可分性

- 熵的定义：
  $$
  \begin{align}
  H(\vec{x})\triangleq H(\vec{p})&=-\sum\limits_{i=1}^cp_ilog\ p_i\\
  &=-\sum\limits_{i=1}^cp(w_i|\vec{x})log\ p(w_i|\vec{x})
  \end{align}
  $$

- 基于熵的可分性测度说明

  1. 熵越大，越不确定，越难分类
  2. 等概率时，熵越大，越难分类





## 三、基于类内散布矩阵的单类模式特征提取



### 1、基本思路

- **类内距离变小**

  要基于类内距离提取特征，就要使类内距离变小

- **迹变小**

  类内距离是协方差矩阵的迹，迹变小，类内距离才能变小

- **特征值之和变小**

  矩阵的迹为所有特征值之和，因此，可以去掉部分特征值，就可以使迹变小，从而使类内距离变小

- **去掉大的特征值**

  大的特征值代表方差大的分量，方差不易分类。同时去掉特征值可以使类内距离变小



### 2、具体做法

- **计算协方差矩阵**

  类内的样本均值为：

  $M=E\{X\}$

  类内散布矩阵（协方差矩阵）为：$C=E\{(X-M)(X-M)^T\}$，样本为$n$维向量，则协方差矩阵为$n*n$维矩阵

- **计算特征值**

  解方程$|\lambda I-C|=0$得到$n$个特征值$\lambda_1,\lambda_2,\dots,\lambda_n$，将$n$个特征值排序，并选取$m（m<n）$个较小的特性值$\lambda_1\le\cdots\le\lambda_m\le\cdots\lambda_n$

- **计算特征向量**

  解方程$(\lambda_kI-C)u'_k=0,k=1,2,\dots,m$得到$m$个特征值对应的$n$维特征向量$u'_k$，对$u'_k$归一化得：
  $$
  u_k=(u_{k1},u_{k2},\dots,u_{kn})^T,k=1,2,\dots,m
  $$

- **特征提取**

  计算$X^*=AX$，其中$A=(u_1,u_2,\dots,u_m)^T$，则$X^*$维新的特征





### 3、特点讨论

- **特征不变**

  使用矩阵$A$对样本矩阵做变换后，新类的特征值不变，不影响分类

- **类内距离变小**

  由于去掉了部分特征值，因此类内距离变小，样本更加集中

- **便于分类**

  去掉了方差大的特征值，从而使得分类更加方便





## 四、基于K-L变换的多类模式特征提取



### 1、基本概念

- **全称**

  $Karhunen-Loeve$变换(卡洛南-洛伊变换)、PCA变换

- **研究对象**

  多类

- **目标**

  在尽量多的保留可分性信息的情况下降维

- **基本思想**

  从样本中提取矩阵，并将矩阵正交化(特征向量)，然后取大的特征值对应的特征向量，对样本进行变换



### 2、基本步骤

- **求样本集$\{X\}$的某个矩阵**$S$

  这些矩阵可以为：

  1. 自相关矩阵

     $R=E[XX^T]=\frac1N\sum\limits_{j=1}^NX_jX_j^T$

     $X是n维样本，包含M类，N是样本数$

  2. 多类类内散布矩阵

     $S_w=\sum\limits_{i=1}^cp(w_i)\frac1{n_i}\sum\limits_{k=1}^{n_i}(X_k^i-M_i)(X_k^i-M_i)^T$

  3. 类间散布矩阵

     $S_b=\sum\limits_{i=1}^cp(w_i)(M_i-M_0)(M_i-M_0)^T$
     
  4. 总体散布矩阵
  
     $S_t=S_b+S_w$

- **求矩阵$S$的特征值**

  $j=1,2,\dots,n$：
  $$
  |\lambda I-S|=0
  $$
  选择$d$个较大的特征值

- **计算$d$个特征向量**
  $$
  (\lambda_kI-S)u'_k=0,k=1,2,\dots,d
  $$

- **计算$X^*=AX$**

  - 其中$A=(u_1,\dots,u_d)^T$，$X^*$为新的特征
  
  
  - $u_i$为$u_i'$归一化后的结果



### 3、特点讨论



- **优点**

  1. **保留了特征**

     K-L变换在在均方误差最小的意义下，使样本集$\{X^*\}$逼近原样本集$\{X\}$的分布，既压缩了维数，又保留了类别鉴别信息。

  2. **突出了类间差异**

     变换后的新模式向量相对总体均值的方差等于原样本集总体自相关矩阵的大特征值（大特征值代表类间差异）

  3. **消除了相关性**

     变换后，样本各分量互不相关，消除了特征之间的相关性。

- **缺点**

  1. **类别多时，效果差**

     K-L变换对两类问题效果好，类别越多效果越差。

  2. **需较多样本**

     采用K-L变换需足够多的样本。当样本数不足时，矩阵的估计会变得粗糙，变换的优越性就不能充分的体现

  3. **计算较慢**

     计算矩阵的特征值和特征向量缺乏统一的快速算法。





## 五、特征选择



### 1、基本概念

- **定义**

  所谓特征选择，就是从$n$个度量值集合$\{x_1,x_2,\dots,x_n\}$中，按某一准则选取出提供分类用的子集，作为降维($m维，m<n$)的分类特征

- **主要问题**

  - 选择的准则：如何衡量选择的特征的好坏
  - 选择的方法：如何选择特征，并获得最优解



### 2、特征选择的准则

包括散布矩阵准则和概率分布准则



#### 散布矩阵准则

- 直接使用散布矩阵作为可分性测度
- 使用散布矩阵组合构建可分性测度

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191206155656768.png" alt="image-20191206155656768" style="zoom: 50%;" />



#### 概率分布准则

- 散度
- Bhattacharyya判据
- Chernoff判据



### 3、特征选择的方法



#### 穷举法

- **基本做法**

  计算出所有可能特征组合的测度，进行比较，并选择处最优特征组合
  
- **可能的特征组合数**

  从$n$个特征中挑选出$d$个，得到的所有可能的特征子集数为：
  $$
  C^d_n=\frac{n!}{(n-d)！d!}
  $$





#### 分支定界算法

- **原理**

  如果可分性测度对于维度是单调的，即从$n$个特征中选择$m$个特征，再从$m$个特征中选择$k$个特征，有$J_n\ge J_m \ge J_k$

- **做法**

  1. 构建一个搜索树
  2. 沿着树自上而下，从右到左搜索可能的组合
  3. 分支定界策略：如果某个节点的准则函数变小，则其下面的子节点准则函数一定小，故可以不搜索其子节点

**例子**

从6个特征中选择两个为例构建树：

![image-20191207145117414](C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191207145117414.png)



**构建树的一般步骤**

1. **计算树的层次**

   假设需要从$n$个特征中选择$d$个特征，则每层需要舍弃一个特征，故树的层次为为$n-d$层

2. **确定每个节点舍弃的特征**

   - 每个节点只能舍弃父节点允许舍弃的一个特征
   - 每个节点只能舍弃"同一父节点下左边子树尚未舍弃的特征"

3. **确定每个节点的子树可舍弃的特征**

   每个节点的子树可以舍弃的特征集是<u>父节点可舍弃的特征集，减去父节点本身舍弃的一个特征，再减去同一父节点下左边兄弟节点舍弃的特征</u>

4. **确定同一父节点下最右边子树的可舍弃数**

   假设最终需要保留$d$个特征，则最右边子树无分支，可舍弃的数目不超过$d+1$



**搜索树的步骤**

- **总体原则**

  搜索过程是从上至下，从右至左进行

- **步骤**

  1. 向下搜索：从最右尚未搜索的分支搜索
  2. 更新测度值：当前的测度值$B$初始化为$0$，若某节点测度值$J$大于$B$，则更新$B$
  3. 向上回溯：回溯到有分支的那个节点则停止回溯
  4. 再向下搜索：停止回溯后，转入向下搜索最右尚未搜索的分支

- **注意**

  搜索过程中，先判断该节点的$J$值是否比$B$值大。若不大于$B$值，该节点以下的各个子结点$J$值均不会比$B$大，故无需对该子树继续进行搜索



#### 单独最优特征组合

- **基本思想**

  计算各个特征单独使用时的判据值并以递减排序，选取当前$d$个分类效果最好的特征

- **要求**

  判据时可分的时，才能选出最优特征，即：
  $$
  J(\vec x)=\sum\limits_{i=1}^nJ(x_i)\ \ 或者\ \ J(\vec x)=\prod\limits_{i=0}^n\ J(x_i)
  $$



#### 顺序前进法

- **基本思想**

  每次从未入选的特征选择一个特征，使得它与已入选的特征组合在一起，所得的$J$值最大，直到特征数增加到要求的数目$d$为止

- **具体步骤**

  1. 设已入选的$k$个特征，构成特征组$X_k$

  2. 把未入选的$(n-k)$个特征按照组合后$J$值大小排队:
     $$
     J(X_k+x_1)\ge\dots\ge J(X_k+x_{n-k})
     $$

  3. 选择下一个特征组:
     $$
     X_{k+1}=X_k+x_1
     $$

- **主要问题**

  一旦某个特征入选后，即使后选的特征使它变得多余，也不能再删除

- **推广**

  可以每次增加$r$个特征



#### 顺序后退法

- **基本思想**

  每次删除一个特征，使得剩下的特征组合在一起时，所得$J$值最大，直到特征数减少到要求的数目$d$为止

- **具体步骤**

  1. 设已删除$k$个特征，剩下的特征组为$\overline {X_k}$

  2. 把$\overline {X_k}$的$(n-k)$个特征按照组合后$J$值大小排队：
     $$
     J(\overline {X_k}-x_1)\ge\dots\ge J(\overline {X_k}-x_{n-k})
     $$

  3. 选择下一个特征为:
     $$
     \overline {X_{k+1}}=\overline {X_k}-x_1
     $$

- **主要问题**

  1. 高维运算计算量大
  2. 一旦剔除就不能再入选

- **推广**

  可以每次减少$r$个特征



#### 增$l$减$r$法

- **基本思想**

  循序前进和顺序后退法的结合，忽而前进，忽而后退

- **具体步骤**

  假设已入选$k$个特征，则先用顺序前进法选择$l$个特征，再用顺序后退法剔除$r$个特征

- **变体**

  当$l>r$时，初始特征数为$0$，先前进，再后退，

  当$l<r$时，初始特征数为$n$，先后退再前进

- **推广**

  也可以每次增减多步





