# 第三章 判别函数和几何分类法



## 一、判别函数



### 1.1 判别函数定义

- 直接用来对模式进行分类的准则函数
- 若分属于ω1，ω2的两类模式可用一方程d(X) =0来划分，那么称d(X) 为判别函数，或称判决函数、决策函数。



#### 前提条件

- 样本所属类别未知
- **样本类别数目已知**



#### 判别函数目标

- 根据判别函数的计算结果直接将模式样本分类



## 二、线性判别函数



### 2.1 线性判别函数的一般形式



<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191025142728439.png" alt="image-20191025142728439" style="zoom:80%;" />



### 2.2 分类方法



#### 两类情况

$$
d(X)=W^TX
$$

- $d(X)>0,X\in w_1$
- $d(X)<0,X\in w_2$
- $d(X)=0，不可判别情况，X\in w_1或X\in w_2$



#### 多类情况

对于M个线性可分模式类,$w_1,w_2,\dots,w_M$,有三种分类方法：

- $w_i/\overline{w_i}$两分法
- $w_i/w_j$两分法
- $w_i/w_j$两分法特例



##### **$w_i/\overline{w_i}$   两分法**

 用线性判别函数将属于$ω_i$类的模式与其余不属于$ω_i$类的模式分开。
$$
d_i(X)=W^TX
$$

- $d(X)>0,X\in w_i$
- $d(X)<0,X\in \overline{w_i}$
- $i=1,2,\dots,M$

- 将某个待分类模式 $X$ 分别代入 M 个类的$d (X)$中，若只有$d_i(X)>0$，其他$d(X)$均$<0$，则判为$ω_i$类。



**示意图**

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191025145205740.png" alt="image-20191025145205740" style="zoom:80%;" />



##### $w_i/w_j$  两分法

- 判别函数：$d_{ij}(X)=W_{ij}^TX,其中d_{ji}=-d_{ij}$
- 判别函数的性质：$d_{ij}(X)>0,对于任意的j\neq i,i、j=1，2，\dots,M，若X\in w_i$

- 在M类模式中，以下标$i$开头的M-1个判别函数全为正时，$X\in w_i$.若其中有一个为负，则为$IR$区

- 判别函数的个数$C_M^2=\frac{M(M-1)}{2!}$



**示意图**

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191025150654354.png" alt="image-20191025150654354" style="zoom:80%;" />



##### $w_i/w_j$  两分法特例

- $w_i/w_j$两分法中的判别函数$d_{ij}(X),$如果可以分解为

$$
d_{ij}(X)=d_i(X)-d_j(X)\\
则\\
d_{ij}(X)>0\to d_i(X)>d_j(X)
$$

- 对于具有判别函数$d_i(X)=W_i^TX,i=1,2\dots,M$的$M$类情况，判别函数性质为：
  - $若X\in w_i,d_i(X)>d_j(X),对于任意的j\neq i,i、j=1,2,\dots,M$
  - $若X\in w_i,d_i(X)=\max\{d_k(X),k=1,2\dots,M\}$

- 简单判别法：**直线相交的钝角区域**

**示意图**

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191025152926524.png" alt="image-20191025152926524" style="zoom:80%;" />

#### 三类方法比较

|                          | $w_i/\overline{w_ i}$                  | $w_i/w_j$                                       | $w_i/w_j$特例                                   |
| ------------------------ | -------------------------------------- | ----------------------------------------------- | ----------------------------------------------- |
| 分类前提                 | 样本线性可分                           | 样本线性可分                                    | 样本线性可分                                    |
| 关键问题                 | 确定系数$W_k$                          | 确定系数$W_k$                                   | 确定系数$W_k$                                   |
| 性能                     | 对于*M*类模式的分类，需要*M*个判别函数 | 对于*M*类模式的分类，需要*M*(*M*-1)/2个判别函数 | 对于*M*类模式的分类，需要*M*(*M*-1)/2个判别函数 |
| 对模式的线性可分的可能性 | 小（有待定区）                         | 大（无待定区）                                  | 大（无待定区）                                  |



## 三、广义线性判别函数



### 3.1 基本思路

- 通过某映射，把模式空间$X$变成$X^*$，以便将$X$空间中非线性可分的模式集，变成在$X^*$空间中线性可分的模式集。



### 3.2 非线性多项式函数

- 设以训练用模式集，$\{X\}$在模式空间$X$中线性不可分，由非线性多项式构成的非线性判别函数形式如下：

$$
d(X)=w_1f_1(x)+w_2f_2(x)+\dots+w_kf_k(x)+w_{k+1}=\sum\limits_{i=1}^{k+1}w_if_i(X)
$$

- 式中$\{f_i(X),i=1,2,\dots,k\}$是模式$X$的单值实函数，$f_{k+1}(X)=1$
- $f_i(X)$取什么形式以及$d(X)$取多少项，取决于非线性边界的复杂程度



### 3.3 广义线性判别函数

- 广义形式的模式向量定义为：

$$
\begin{align}
X^*&=[x_1^*,x_2^*,\dots,x_k^*,1]\\
&=[f_1(X),f_2(X),\dots,f_k(X),1]
\end{align}
$$

- 这里$X^*$空间的维数$k$高于$X$空间的维数n，其中

$$
\begin{align}
&d(X)=W^TX^*=d(X^*)\\
&W=[w_1,w_2,\dots,w_k,w_{k+1}]^T
\end{align}
$$

上面的式子是线性的，故称为广义线性判别函数





## 四、线性判别函数的几何性质



### 4.1 模式空间与超平面



#### 基本概念

- 模式空间：以n维模式向量$X$的$n$个分量为坐标变量的欧氏空间
- 模式向量的表示：点、有向线段
- 线性分类：用$d(X)$进行分类，相当于用超平面$d(X)=0$把模式空间分成不同的决策区域
- 超平面：设判别函数为$d(X)=W_0^TX+w_{n+1}$，则超平面定义为：

$$
d(X)=W_0^TX+w_{n+1}=0
$$



#### 法向量

假设模式向量$X_1$ 和$X_2$在超平面上，则：
$$
\begin{align}
&W_0^TX_1+w_{n+1}=W_0^TX_2+w_{n+1}\\
&W_0^T(X_1-X_2)=0
\end{align}
$$
**法向量的定义**

上式表明$W_0$与超平面上任意向量正交，即垂直于超平面，称为超平面的法向量，方向由超平面的负侧指向正侧

**key**

- 超平面的法向量就是判别函数的权向量
- 超平面的位置由$w_{n+1}$决定

**单位法向量U**

$U=\frac{W_0}{||W_0||}$，其中$||W_0||=\sqrt{w_1^2+w_2^2+\dots+w_n^2}$

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191025195153827.png" alt="image-20191025195153827" style="zoom:80%;" />



#### 向量X判别函数$d(x)$ 的取值

- $X$在超平面上时，取值为0
- $X$不在超平面上时，将X向超平面投影得到向量$X_p$，构造向量$R:R=r\cdot U=r\frac{W_0}{||W_0||}$
- $r:X$到超平面的代数距离

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191025195945084.png" alt="image-20191025195945084" style="zoom:50%;" />
$$
\begin{align}
X&=X_p+R=X_p+r\frac{W_0}{||W_0||}\\
d(X)&=W_0^T(X_p+r\frac{W_0}{||W_0||})+w_{n+1}\\
&=(W_0^TX_p+w_{n+1})+W_0^T\cdot r\frac{W_0}{||W_0||}\\
&=r||W_0||
\end{align}
$$

- **key:**判别函数$d(X)$的取值正比于点到超平面的距离



#### 向量到超平面距离的计算

- $X$到超平面的距离：$r=\frac{d(X)}{||W_0||}$
- $d(X)>0$时，X在超平面正侧
- $d(X)<0$时，$X$在超平面负侧
- $X$在原点时，到超平面的距离：

$$
\begin{align}
&因为d(X)=W_0^TX+w_{n+1}\\
&所以r_0=\frac{w_{n+1}}{||W_0||}
\end{align}
$$

- $w_{n+1}>0$时，原点在超平面的正侧
- $w_{n+1}<0$时，原点在超平面的负侧
- $w_{n+1}=0$时，超平面通过原点



### 4.2 权空间与权向量



#### 权空间概念



**权空间**

以线性判别函数的权值为坐标变量的$n+1$维欧氏空间称为权空间

- 增广权向量：$w=(w_1,w_2,\dots,w_n,w_{n+1})^T$对应权空间的一个点，或原点到该点的有向线段



**规范化增广样本向量**

对于两类问题：$w_1=\{w_{11},w_{12},\dots,w_{1p}\}$，$w_2=\{w_{21},w_{22},\dots,x_{2q}\}$，令：
$$
\begin{align}
X&=X_{1i},i=1,2\dots,p\\
&=-X_{2i},i=1,2,\dots,q\\
\end{align}
$$
则$X$称为规范化增广样本向量



#### 权向量解



**样本规范化后的判别函数**

- 样本规范化后，对所有的$p+q$个样本，其判别函数统一为$d(X)=W^TX>0$



**权向量的解区**

- 要将上述两类分开，则对$p+q$个样本，其判别函数都大于0，即$p+q$个不等式成立，这些不等式围成的区域就是$W$的解区



**权向量的解**

- 权向量解区中的任意一点都是权向量的解



## 五、感知器算法



### 5.1 基本概念



#### 学习与训练

**学习**

从分类器的角度讲：

- 非监督学习（聚类）
- 监督学习（训练）



**训练**

用已知类别的模式样本指导机器对分类规则进行反复修改，最终使分类结果与已知类别信息完全相同



#### 确定性分类器

处理确定可分情况的分类器，通过几何方法将特征空间分解为对应不同类的子空间，又称为几何分类器



#### 感知器

- 是一种神经网络，对一种分类学习机模型的称呼
- 由于无法实现非线性分类而下马，但赏罚概念得到广泛应用



### 5.2 算法描述

感知器算法通过对已知类别的训练样本集的学习，寻找一个满足要求的权向量。



#### 输入

- 两类线性可分的模式类$w_1,w_2$
- 训练样本集
- 判别函数$d(X)=W^TX$，其中：
  - $W=[w_1,w_2,\cdots,w_n,w_{n+1}]^T$
  - $X=[x_1,x_2,\cdots,x_n,1]^T$



#### 输出

权向量$W$，使得$d(X)=W^TX$：

- 若$X\in w_1,d(X)>0$
- 若$X\in w_2,d(X)<0$



#### 步骤

**1、初始化**

- 选择N个分属于$w_1$和 $ω_2$ 类的模式样本构成增广向量形式训练样本集$\{ X_1,  …, X_N \}$
- 并进行规范化处理，即$ω_2$类样本全部乘以(－1) ，规范化后有$d(X)=W^TX>0$
- 任取权向量初始值W(1)，开始迭代。迭代次数k=1 。



**2、迭代**

用全部训练样本进行一轮迭代，计算$W^T(k)X_i$的值，并分两种情况修正权向量：

- 若$W^TX_i\le 0$，分类器对第$i$个模式做了错误的分类，权向量校正为：$W(k+1)=W(k)+cX_i$，其中$c$为正的矫正增量
- 若$W^T(k)X_i>0$，分类正确，权向量不变，$W(k+1)=W(k)$



**3、分析分类结果**

只要有一个错误分类，回到第二步，直至所有样本正确分类



#### 算法核心思想

感知器算法是一种赏罚过程。分类正确时，对权向量“赏”—即权向量不变；分类错误时，对权向量“罚”—对其修改，向正确的方向转换。



#### 算法的收敛性

可以证明感知器算法是收敛的。收敛条件：模式类别线性可分。即经过算法的有限次迭代运算后，一定能求出一个使所有样本都能正确分类的$W$。



### 5.3 感知器算法用于多类情况



#### 多类情况3回顾

- 若$X\in w_i$,则$d_i(X)>d_j(x),\forall j\ne i,j=1,\cdots,M$
- 对于$M$类模式应存在$M$个判决函数：$\{d_i,i=1,2\cdots,M\}$



#### 输入

- $M$种模式类别：$w_1,w_2,\cdots,w_M$
- 样本数据集
- 判决函数$\{d_i,i=1,\cdots,M\}$



#### 输出

$M$个判决式的权向量$W$，使得：

- $若X\in w_i,则d_i(X)>d_j(X)，\forall j\ne i,j=1,\cdots,M$



#### 算法描述



**1、初始化**

- 设权向量初值为：$W_j(X),j=1,\cdots,M$
- 训练样本为增广向量形式，<u>但不需要规范化处理</u>



**2、迭代**

第$k$次迭代时，一个属于$w_i$类的模式样本$X$被送入分类器，计算所有判别函数$d_j(k)=W_j^T(k)X,j=1,\cdots,M$，分两种情况修改权向量：

- 若$d_i(k)>d_j(k),\forall j\ne i,j=1,2,\cdots,M，则权向量不变：$
  - $W_j(k+1)=W_j(k),j=1,2,\cdots,M$
- 若第$l$个权向量使$d_i(k)<d_j(k)$，则相应的权向量做调整，即：
  - $W_i(k+1)=W_i(k)+cX$
  - $W_l(k+1)=W_l(k)-cX$
  - $W_j(k+1)=W_j(K),j\ne i,l$
  - c为正的校正增量





#### 核心思想

赏罚过程



#### 收敛性

只要模式类在情况3判别函数时是可分的，则经过有限次迭代后算法收敛



## 六、梯度算法



### 6.1 基本原理



#### 梯度概念



**梯度定义**

设函数$f(Y)$是向量$Y=[y_1,\cdots,y_i,\cdots,y_N]^T$的一个标量函数，则$f(Y)$的梯度定义为：
$$
\nabla f(Y)=\frac{d}{dY}f(Y)=[\frac{\partial f}{\partial y_1},\frac{\partial f}{\partial y_2},\cdots,\frac{\partial f}{\partial y_n}]^T
$$


#### 梯度定义解释

- 梯度是向量，它的分量$\frac{\partial f}{\partial y_i}$表示函数$f(Y)$在自变量$y_i$方向上的变化速度
- 梯度是方向，是自变量增加时，$f(Y)$增长最快的方向
- 沿着梯度方向走，会到达极大值
- <u>沿着负梯度方向走，会到达极小值（求函数极小值的方法）</u>



#### 梯度算法解决的问题

设两个线性可分的模式类$w_1$和$w_2$的样本共$N$个，$w_2$类样本乘(-1)，将两类样本分开的判决函数$d(X)$应满足：

- $d(X_i)=W^TX_i>0,i=1,2,\cdots,N$
- 权向量$W$的解区由$N$个不等式确定



<u>梯度算法解决的问题是：求一个满足要求的权向量</u>



#### 梯度算法的基本思想

使用<u>梯度下降的方法求准则函数的极小值</u>（即利用负梯度向量的值对权向量$W$进行修正，使准则函数达到极小值）



### 6.2 算法描述



#### 输入

- 两类线性可分的模式类$w_1,w_2$
- 训练样本集
- 判决函数$d(X)=W^TX$，其中：
  - $W=[w_1,w_2，\cdots,w_n,w_{n+1}]^T$
  - $X=[x_1,x_2,\cdots,x_n,1]^T$



#### 输出

权向量$W$，使得$d(X)=W^TX$：

- $若X\in w_1,d(X)>0$
- $若X\in w_2,d(X)<0$



#### 步骤

**1、初始化**

- 选择$N$个分属于$w_1$和$w_2$类的模式样本构成增广向量形式训练样本集$\{X_1,\cdots,X_N\}$
- 进行规范化处理，即$w_2$类的样本全部乘以-1，规范化后有$d(X)=W^TX>0$
- 任取权向量初始值$W(1)$，<u>选择准则函数</u>并开始迭代



准则函数$J(W,X)$的选取规则：

- 具有唯一的最小值，并且这个最小值发生在$W^TX_i>0$
- 对错误分类敏感



**2、迭代**

依次输入训练样本集$X$，设第$k$次迭代时输入样本为$X_i$，此时已有权向量$W(K)$，使用下面的公式求梯度$\nabla J(k)$
$$
\nabla J(k)=\frac{\partial J(W,X_i)}{\partial W}|_{W=W(k)}
$$


使用梯度修改权向量（$c$为步长，正值）：
$$
W(k+1)=W(k)-c\nabla J(K)
$$
迭代次数$k$加1，输入下一个训练样本，计算新的权向量，直至对所有训练样本完成一轮训练



**3、分析分类结果**

如果有一个样本使$\nabla J\ne0$，回到第二步进行下一轮迭代，否则，$W$不再变化，算法收敛



#### 讨论

$$
\begin{align}
W(k+1)&=W(k)-c\nabla J(K)\\
&=W(k)-c\frac{\partial J(W,X)}{\partial W}|_{W=W(k)}
\end{align}
$$

**收敛性**

随着权向量$W$向理想值接近，准则函数关于W的导数 ($\nabla J$)越来越趋近于零，这意味着准则函数$J$ 越来越接近最小值。当最终$\nabla J=0$时，$J$达到最小值，此时$W$不再改变，算法收敛。



**步长c**

 c值的选择很重要，如c值太小，收敛太慢；但若太大，搜索又可能过头，甚至引起发散。



**准则函数**

梯度算法利用准则函数$J(W, X)$来调节权向量，$J(W, X)$的形式不同，算法也不同。



### 6.3 固定增量算法



#### 基本流程

**输入**

- $X=[x_1,x_2,\cdots,x_n,1],W=[w_1,w_2,\cdots,w_n,w_{n+1}]$

- 准则函数：$J(W,X)=\frac{1}{2}(|W^TX|-W^TX)$
- 准则函数具有唯一最小值0，且发生在$W^TX>0$的时候



**输出**

能正确分类的权向量



**算法**

梯度算法



**关键问题**

- 计算准则函数的梯度
- 调节权向量



#### 准则函数的梯度



**数学基础**

函数对向量求导等于函数对向量的分量求导，即：
$$
\frac{\partial f}{\partial X}=[\frac{\partial f}{\partial x_1},\cdots,\frac{\partial f}{\partial x_n}]^T
$$

**准则函数：**
$$
J(W,X)=\frac{1}{2}(|W^TX|-W^TX)
$$

**1、求$W^TX$部分**
$$
\begin{align}
\frac{\partial (W^TX)}{\partial W}&=\frac{\partial}{\partial W}(\sum\limits_{i=1}^{n}w_ix_i+w_{n+1})\\
&=[\frac{\partial}{\partial w_1}(\sum\limits_{i=1}^{n}w_ix_i+w_{n+1}),\cdots,\frac{\partial}{\partial w_k}(\sum\limits_{i=1}^{n}w_ix_i+w_{n+1}),\frac{\partial}{\partial w_{n+1}}(\sum\limits_{i=1}^{n}w_ix_i+w_{n+1})]^T\\
&=[x_1,\cdots,x_k,\cdots,x_n,1]^T\\
&=X
\end{align}
$$


**2、求|$W^TX$|部分**
$$
\begin{align}
&W^TX>0时，\frac{\partial(|W^TX|)}{\partial W}=\frac{\partial(|W^TX|)}{\partial W}=X\\
&W^TX\le0时，\frac{\partial(|W^TX|)}{\partial W}=\frac{\partial(-W^TX)}{\partial W}=-X\\
&所以：\\
&\frac{\partial(|W^TX|)}{\partial W}=[sgn(W^TX)]\cdot X\\
&其中：\\
&sgn(W^TX)=+1，若W^TX>0\\
&sgn(W^TX)=-1，若W^TX\le0
\end{align}
$$


**3、求$\nabla$J**
$$
\nabla J=\frac{\partial J(W,X)}{\partial W}=\frac{1}{2}[Xsgn(W^TX)-X]
$$



#### 调节权重

将$\nabla J=\frac{\partial J(W,X)}{\partial W}=\frac{1}{2}[Xsgn(W^TX)-X]$代入$W(k+1)=W(k)-c\nabla J=W(k)-c[\frac{\partial J(W,X)}{\partial W}]_{W=W(k)}$

得到：
$$
\begin{align}
W(k+1)&=W(k)-c\frac12[Xsgn(W^T(k)X)-X]\\
&=W(k)+\frac c2[X-Xsgn(W^T(k)X)]\\
&=W(k)+cX,若W^TX\le 0\\
&=W(k),若W^TX>0
\end{align}
$$
即：

$W(k+1)=W(k),当W^T(k)X>0$

$W(k+1)=W(k)+cX$，当$W^T(k)X\le 0$





## 七、最小平方误差算法

LMSE算法，对线性可分的模式收敛，能检测不可分的模式



### 7.1 基本原理

#### 核心思想

对于两类问题的训练样本集${X_i, i=1,…,N}$($X_i$为规范化增广向量)，其权重W应该满足$W^TX_i$>0，写成矩阵形式即：
$$
\left(\begin{array}{ccccc}
x_{11}&x_{12}&\ldots&x_{1n}&1  \\
x_{21}&x_{22}&\ldots&x_{2n}&1	\\
\vdots&\vdots&\vdots&\vdots&\vdots\\
-x_{N1}&-x_{N2}&\ldots&-x_{Nn}&-1
\end{array} \right)_{NX(n+1)}
\left(\begin{array}{c}
w_1\\
w_2\\
\vdots \\
w_n\\
w_{n+1}
\end{array}\right)_{(n+1)X1}
>
\left(\begin{array}{c}
0\\
0\\
\vdots \\
0\\
0
\end{array}\right)_{NX1}=0
$$
令上述NX(n+1)的长方矩阵为X，则上面的矩阵可以写成：$XW>0$

**感知器算法的核心就是调节权重$W$使得不等式$XW>0$成立**





#### 基本做法

- LMSE算法把对$XW>0$的求解，改为对$XW=B$求解
- 由于$B=[b_1,b_2,\ldots,b_i,\ldots,b_N]^T$各分量均为正值，故两者等价



#### 问题

由于样本数$N$通常大于维度$n$，即矩阵的行远大于矩阵的列，$XW=B$为矛盾方程组，只能求近似解。即求最小二乘近似解$W^*$，使得$||XW^*-B||=极小$



#### 基本思路

选择准则函数$J(W,X,B)=\frac12||XW-B||^2$，使得当$J$达到最小值时，$XW=B$可得到近似解（最小二乘近似解）



#### 准则函数

**准则函数化简**
$$
\begin{align}
||XW-B||^2&=(W^TX_1-b_1)^2+\cdots+(W^TX_N-b_N)^2\\
&=\sum\limits_{i=1}^{N}(W^TX_i-b_i)^2\\
J(W,X,B)&=\frac12||XW-B||^2\\
&=\frac12\sum\limits_{i=1}^N(W^TX_i-b_i)^2
\end{align}
$$


**准则函数的物理意义**

- 方程组$XW=B$的近似解也称为最优近似解





### 7.2 算法描述

#### 输入

- $N$个分属于两类的样本
- 准则函数：$J(W,X,B)=\frac12||XW-B||^2$



#### 输出

$W,B$，可分性判断



#### 步骤

 一、根据$N$个分属于两类的样本，写出规范化增广样本矩阵$X$ 

二、求$X$的伪逆矩阵：$X^{\#}=(X^TX)^{-1}X^T$

三、设定初值$c$和$B(1)$，$c$为正的校正增量，$B(1)$的各分量大于零，迭代次数$k=1$，开始迭代：计算$W(1)=X^\#B(1)\dots$

四、计算$e(k)=XW(k)-B(k)$，进行可分性判别

- 如果$e(k)=0$，线性可分，解为$W(k)$，算法结束
- 如果$e(k)>0$，线性可分，进入下一步使得$e(k)\to 0$，得到最优解
- 如果$e(k)<0$，线性不可分，停止迭代，无解，算法结束
- 否则说明$e(k)$的各个分量值有正有负，进入下一步

五、计算$W(k+1)和B(k+1)$：

- 分别计算$W(k+1)=W(k)+cX^\#|e(k)|和B(k+1)=B(k)+c[e(k)+|e(k)|]$
- 先计算$B(k+1)=B(k)+c[e(k)+|e(k)|]$，再计算$W(k+1)=X^\#B(k+1)$



### 7.3 关键问题



#### 准则函数$J$的梯度

由于存在两个变量，故存在两个梯度：
$$
\begin{align}
\frac{\partial J}{\partial W}&=X^T(XW-B)\\\\
\frac{\partial J}{\partial B}&=-\frac12[(XW-B)+|XW-B|]
\end{align}
$$


#### 调节B

$$
\begin{align}
B(k+1)&=B(k)-c^{'}[\frac{\partial J}{\partial B}]_{B=B(k)}\\
&=B(k)+\frac{c^{'}}2[(XW-B)+|XW-B|]\\
&令\frac{c^{'}}2=c、XW(k)-B(K)=e(k)得到：\\
&B(k+1)=B(k)+c[e(k)+|e(k)|]
\end{align}
$$



#### 调节权向量



**方法一**

- $J$对$W$最小时，梯度为0，即$\frac{\partial J}{\partial W}=X^T(XW-B)=0$

- 由此推出：$W=X^\#B$
- 只需要先调节B，然后利用公式$W=X^\#B$计算$W$的值即可





**方法二**

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191110180514869.png" alt="image-20191110180514869" style="zoom:80%;" />

#### 线性可分判断

判断方法，利用$e(k)$的取值进行判断，$XW(k)-B(k)=e(k)$:

-  如果$e(k)=0$ ，表明$XW(k)=B(k) >0$，有解
-  如果$e(k)>0$  ，表明$XW(k)>B(k) >0$ ，隐含有解。继续迭代，可使e(k) →0 
- 如果$e(k)<0$（所有分量为负数或零，但不全为零），停止迭代，无解。此时若继续迭代，数据不再发生变化

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191110180946956.png" alt="image-20191110180946956" style="zoom:80%;" />