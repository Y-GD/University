# 第八章	神经网络模式识别法



## 一、人工神经网络的发展概况



### 1、人工神经网络定义

- 人工神经网络简称神经网络，是模拟人脑智能特点和信息处理机制的技术



### 2、人工神经网络的发展历史

- **早期研究**

  1943年提出，是神经网络的先驱

- **基本技术**

  50年代Rosenlatt提出感知器

- **能量函数**

  1982年Rumelhart和McClelland提出著名的BP算法。该算法由后向前修正各层之间的连接权值，从实践上证明了神经网络具有很强的运算能力



### 3、基本结构

神经网络由大量简单的基本元件—神经元相互连接而成的自适应非线性动态系统。每个神经元的结构和功能简单，而大量神经元组合产生的系统行为却非常复杂



## 二、神经网络的基本概念



### 1、生物神经元



#### 基本结构

细胞体（处理）、树突（输入）、轴突和突触（输出）

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219151115419.png" alt="image-20191219151115419" style="zoom:67%;" />



#### 工作机制

三个主要因素：

- 信息输入
- 信息处理
- 信息输出给其他神经元

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219151356959.png" alt="image-20191219151356959" style="zoom:50%;" />





### 2、人工神经元及神经网络



#### 基本结构

- **人工神经元模型：**

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219151741584.png" alt="image-20191219151741584" style="zoom:50%;" />

- **人工神经元间的互联**

  信息传递路径轴突-突触-树突的简化

- **连接的权值**

  两个互连的神经元之间相互作用的强弱



#### 神经元的动作

$$
\begin{align}
&net=\sum\limits_{i=1}^nw_ix_i(x_i,w_i\in R)\\
&y=f(net)
\end{align}
$$

- **输出函数**$f$

  也称作用函数，非线性

  ![image-20191219152229541](C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219152229541.png)



#### 非线性函数f的例子

- **阈值型函数**

$$
f=sgn(\sum\limits_{i=1}^n w_ix_i-\theta)
$$

- **点积形式**

  令$\theta=-w_{n+1}$，则上面函数可以写为：
  $$
  y=sgn(W^TX)
  $$
  其中，$W=[w_1,w_2,\dots,w_n,w_{n+1}]^T\ X=[x_1,x_2,\dots,x_n,1]^T$



### 3、神经网络的学习



#### 基本概念

- **神经网络的学习过程**

  同一个训练集的样本输入输出反复作用于网络，网络按照一定的训练规则自动调节神经元之间的连接强度或者拓扑结构，使实际输出满足期望的要求或者趋于稳定（**即修正权值**）

- **典型的权值修正方法**

  - $Hebb$学习规则
  - 误差修正学习规则($\delta$学习规则)



#### Hebb学习规则

- **神经元间的连接**

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191220150157130.png" alt="image-20191220150157130" style="zoom:50%;" />

- **基本思想**

  如果神经网络中某一神经元与另一直接与其相连的神经元同时处于兴奋状态，那么这两个神经元之间的连接强度应该加强

- **调权过程**
  $$
  \begin{align}
  &w_{ij}(t+1)=w_{ij}(t)+\eta[\ y_j(t)y_i(t)\ ]\\\\
  &w_{ij}(t+1):修正一次后的某一权值\\\\
  &\eta:学习因子，表示学习速率的比例常数\\\\
  &y_j(t),y_i(t):分别表示t时刻第j个和第i个神经元的状态(输出)
  \end{align}
  $$



#### $\delta$学习规则

- **基本步骤**

  1. 选择一组初始的权值$w_{ij}(1)$

  2. 计算某一输入模式对应的实际输出与期望输出的误差

  3. 更新权值，阈值可以视为输入恒为（-1）的一个权值

     $w_{ij}(t+1)=w_{ij}(t)+\eta [d_j-y_j(t)]x_i(t)$

     式中：

     $\eta:$学习因子

     $d_j,y_j(t):$第$j$个神经元的期望输出与实际输出

     $x_i(t):$第$j$个神经元的第$i$个输入

  4. 返回（2）,直到所有训练模式网络输出均能满足要求



### 4、神经网络的结构分类



#### 分层结构

- 有明显层次，信息流由输入层到输出层
- 前馈神经网络



#### 相互连接结构

- 没有明显的层次，任意两个神经元之间可达，具有输出单元到隐层单元或输入单元的反馈连接
- 反馈神经网络





## 三、前馈神经网络



### 1、感知器



#### 基本概念

- **感知器结构示意图**

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219154856987.png" alt="image-20191219154856987" style="zoom:50%;" />

- **感知器的结构特点**

  1. 只有两层（输入层、输出层）
  2. 两层单元之间为全互联
  3. 连接权值可调
  4. 输出层神经元个数等于类别数（两类问题时输出层为一个神经元）



#### 判决过程

- **判决图**

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219155207648.png" alt="image-20191219155207648" style="zoom:50%;" />

- **输入**

  模式向量$X=[x_1,x_2,\dots,x_n]^T$,共$M$类

- **输出**

  $y_j=f(\sum\limits_{i=1}^n\ w_{ij}x_i-\theta_j)$

  1、$\theta_j：$第$j$个神经元的阈值

  2、$w_{ij}：$输入模式第$i$个分量与输出层第$j$个神经元之间的连接权

  3、输出层第$j$个神经元对应第$j$个模式类

  4、令$\theta_j=-w_{(n+1)j}$，取$w_j=[w_{1j},w_{2j},\dots,w_{(n+1)j}]^T$,$X=[x_1,x_2,\dots,x_n,1]^T$,有：
  $$
  y_j=f(\sum\limits_{i=1}^{n+1}w_{ij}x_i)=f(W^T_jX)
  $$

- **判决**

  $M$类问题的判决规则（神经元的输出函数）为：
  $$
  y_j=f(W^T_jX)=\begin{cases}+1,&若X\in w_j\\-1,&若X\notin w_j \end{cases}
  $$
  其中$1\le j\le M $

- **正确判决的关键**

  输出层每个神经元必须有一组合适的权值

- **感知器的关键是通过训练挑权值**





#### 训练调权值的过程

- **关键技术**

  1. 感知器采用监督学习算法得到权值
  2. 权值更新方法，$\delta$学习规则

- **算法步骤**

  1. 设置初始权值$w_{ij}(1),w_{(n+1)j}(1)$为第$j$个神经元的阈值

  2. 输入新的模式向量

  3. 计算神经元的实际输出
     $$
     \begin{align}
     设第&k次输入的模式向量为X_K\\
     1、&与第j个神经元相连的权向量为：\\
     &W_j(k)=[w_{1j},w_{2j},\dots,w_{(n+1)j}]^T\\
     
     2、&第j个神经元的实际输出为：\\
     &y_j(k)=f[W^T_j(k)X_k]\ \ \ \  1\le j\le M
     \end{align}
     $$

  4. 修正权值
     $$
     \begin{align}
     &W_j(k+1)=W_j(k)+\eta[d_j-y_j(k)]X_k\\
     &d_j:第j个神经元的期望输出\\
     &d_j=\begin{cases}+1,&X_k\in w_j\\ -1,&X_k\notin w_j\end{cases}\\
     &1\le j\le M
     \end{align}
     $$

  5. 转到第二步

     当全部学习样本都能正确分类时，学习过程结束。

     经验证明，当$\eta$随着$k$的增加而减小时，算法一定收敛



#### 局限

- **局限**

  要求模式类线性可分，导致其学习能力有限。例如：下面的$XOR$运算，不能有一条直线分类

- **XOR运算的分类方法**

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191219164448740.png" alt="image-20191219164448740" style="zoom:50%;" />

  - 确定两条分界线，每个分界线实现对平面的一个分类
  - 将第一步的结果组合起来，实现正确分类

- 对于需要跟多判决界面的复杂分类，两层感知器无能为力，<u>故需要更多层感知器</u>





### 2、BP网络-多层感知器的设计



#### 多层感知器的设计

- **选定参数**

  包括一个输入层、一个输出层和多个隐藏层

- **输入层**

  输入层节点数为输入特征维数$d$，映射函数采用线性函数

- **隐含层**

  隐含层节点数需要设定，一般来说，隐藏层节点数越多，网络的分类能力越强，映射函数一般采用$Sigmoid$函数

- **输出层**

  输出层节点数可以等于类别数$c$,也可以采用编码输出的方式，少于类别数，输出函数可以采用线性函数或者$Sigmoid$函数



#### 多层感知器的分类

- 隐含层实现对输入空间的非线性映射，输出层实现线性分类
- 非线性映射方式和线性判别函数可以同时学习

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191220154617421.png" alt="image-20191220154617421" style="zoom:50%;" />



#### 多层感知器的判别函数

- **三层感知器**

$$
g_k(x)=f_2(\sum\limits_{j=1}^{n_H}w_{jk}f_1(\sum\limits_{i=1}^dw_{ij}x_i+w_{0j})+w_{0k})
$$

第$k$个输出层神经元的输出，其中$d$为特征维数，$n_H$为隐藏节点数

- **关键思路**

  实现线性和非线性空间映射的结合

- **主要问题**

  只能实现一层的调权，无法对多层同时调权



#### BP算法的基本思路

- **两个阶段**

  正向传播阶段、反向传播阶段

- **正向传播阶段**

  1. 输入数据经每层处理后逐层向后传递结果
  2. 每层的输出只影响下一层，最终达到输出层
  3. 如果输出结果不理想（误差大），则调用反向传播

- **反向传播阶段**

  1. 误差信号按原路返回
  2. 每层按照误差信号修改权值，使误差达到最小



#### BP算法的具体步骤

1. **初始化**

   对权值和神经元阈值初始化：（0，1）上分布的随机数

2. **做标记**

   输入样本，指定输出层各神经元的希望输出值
   $$
   d_j=
   \begin{cases}
   +1,&X\in w_j\\
   -1,&X\notin w_j
   \end{cases}
   \ \ \ j=1,2,\dots,M
   $$

3. **正向传播**

   以此计算每层神经元的实际输出，直到输出层

4. **反向传播**

   从输出层开始修正每个权值，直到第一隐藏层
   $$
   \begin{align}
   &w_{ij}(t+1)=w_{ij}(t)+\eta\ \delta_j y_j,\ \ 0<\eta<1\\
   &若j是输出层神经元，则：\\
   &\delta_j=y_j(1-y_j)(d_j-y_j)\\
   &若j是隐藏层神经元，则：\\
   &\delta_j=y_j(1-y_j)\sum\limits_k\delta_kw_{jk}
   \end{align}
   $$
   
5. 转到第二步，循环至权值稳定为止



#### BP算法正向传播数学描述

设某一神经元$j$的输入为$net_j$，输出为$y_j$；相邻低一层中任意神经元$i$的输出为$y_i$，则有：
$$
net_j=\sum\limits_iw_{ij}y_i\ \ \ \ y_j=f(net_j)
$$
其中：

- $w_{ij}:$神经元$i$与$j$之间连接的权
- $f(\cdot):$神经元的输出函数

**例：**

以$S$型函数为例：

<img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191221104629179.png" alt="image-20191221104629179" style="zoom:50%;" />

- $y_j=f(net_j)=\frac1{1+e^{-(net_j-\theta_j)/h_0}}$
- $\theta_j:$神经元阈值
- $h_0:$修改输出函数形状的参数



#### BP算法误差反向传播的数学描述

- **已知条件**

  训练样本$x$，期望输出$t=(t_1,\dots,t_c)$，网络实际输出$z=(z_1,\dots,z_c)$，隐藏层输出$y=(y_1,\dots,y_{n_H})$，第$k$个神经元的净输出$net_k$

- **求解的问题**

  调节各层的均值

- **方法**

  定义一个准则函数，求准则函数对权值的梯度最小

- **准则函数**

  $J(W)=\frac12||t-z||^2=\frac12\sum\limits_{i=1}^c(t_i-z_i)^2$

- **调权公式**

  $w(m+1)=w(m)+\Delta w(m)=w(m)-\eta\frac{\partial J}{\partial w}$

- **输出层梯度计算**

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191221105913508.png" alt="image-20191221105913508" style="zoom: 50%;" />
  $$
  \begin{align}
  &\frac{\partial J}{\partial w_{kj}}=\frac{\partial J}{\partial net_k}\frac{\partial net_k}{\partial w_{kj}}\\
  &net_k=\sum\limits_{i=1}^{n_H}w_{ki}y_i\ \ \ \ \ \ \frac{\partial net_k}{\partial w_{kj}}=y_j\\
  &\frac{\partial J}{\partial net_k}=\frac{\partial J}{\partial z_k}\frac{\partial z_k}{\partial net_k}=-(t_k-z_k)f'(net_k)\\
  &所以：\\
  &\frac{\partial J}{\partial w_{kj}}=-(t_k-z_k)f'(net_k)y_j=-\delta_ky_j\\
  &\delta_k=(t_k-z_k)f'(net_k)
  \end{align}
  $$

- **隐含层梯度计算**

  <img src="C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\image-20191221111051595.png" alt="image-20191221111051595" style="zoom:50%;" />
  $$
  \begin{align}
  &\frac{\partial J}{\partial w_{ji}}=\frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial net_j}\frac{\partial net_j}{\partial w_{ji}}\\
  &\frac{\partial y_j}{\partial net_j}=f'(net_j)\\
  &\frac{\partial net_j}{\partial w_{ji}}=\frac{\partial}{\partial w_{ji}}(\sum\limits_{m=1}^dw_{jm}x_m)=x_i\\
  \end{align}
  $$

  $$
  \begin{align}
  \frac{\partial J}{\partial y_j}&=\frac{\partial}{\partial y_j}[\frac12\sum\limits_{k=1}^c(t_k-z_k)^2]\\
  &=-\sum\limits_{k=1}^c(t_k-z_k)\frac{\partial z_k}{\partial y_j}\\
  &=-\sum\limits_{k=1}^c(t_k-z_k)\frac{\partial z_k}{\partial net_k}\frac{\partial net_k}{\partial y_j}\\
  &=-\sum\limits_{k=1}^c(t_k-z_k)f'(net_k)w_{kj}\\
  所以：\\
  \frac{\partial J}{\partial w_{ji}}&=-[\sum\limits_{k=1}^c(t_k-z_k)f'(net_k)w_{kj}]f'(net_j)x_i\\
  \frac{\partial J}{\partial w_{ji}}&=-\delta_jx_i,\ \ \ \ \delta_j=f'(net_j)\sum\limits_{k=1}^c \delta_k w_{kj}
  \end{align}
  $$

- **总结**
  $$
  \begin{align}
  调权公式:&\\
  w(m+1)&=w(m)+\Delta w(m)=w(m)-\eta\frac{\partial J}{\partial w}\\
  其中：&\\
  输出层梯度：&\\
  \frac{\partial J}{\partial w_{kj}}&=-\delta_ky_j，\ \ \delta_k=(t_k-z_k)f'(net_k)\\
  隐含层梯度：&\\
  \frac{\partial J}{\partial w_{ji}}&=-\delta_jx_i，\ \ \delta_j=f'(net_j)\sum\limits_{k=1}^c\delta_kw_{kj}
  
  \end{align}
  $$
  

#### BP网络存在的问题

- 收敛速度慢
- 只能收敛于局部最优解，不能保证收敛于全局最优解
- 泛化能力差。当隐藏层数量足够多时，对训练样本识别率高，但是对测试样本识别率低



