## 第二章 熵、相对熵与互信息



## 一、信息的度量

- 信源发出的消息常常是随机的，其状态存在某种程度的不确定性，经过通信将信息传给了收信者，收信者得到消息后，才消除了不确定性并获得了信息。
- 获得信息量的多少与信源的不确定性的消除有关
- 不确定度——惊讶度——信息量



### 信息的度量与概率的关系

- 不确定度与样本空间和对应概率有关系
- 等概率的不确定度比其他分布要大
- 信息量的数学表示： $I(p)=log_2\frac{1}{p}$



## 二、熵



### 1、定义

- Entropy,原子热力学概念，熵代表系统的混乱程度的增加

$$
dS=\frac{dQ}{T}
$$

- 数学定义


$$
H(x)=-\sum \limits_{x}p(x)log\ p(x)
$$

- 信息量的期望
  $$
  H(X)=-E\ log\ p(X)
  $$



### 2、引理

- $H(X)\geq 0$
- $H_b(x)=log_b\ aH_a(X)$



### 3、单位

- Bit—base 2
- Nat—base e
- Dit/hat—base 10



### 4、二元熵函数

- $h_b(\gamma)=-\gamma log\ \gamma\ -(1-\gamma)log(1-\gamma)$

- $0\leq\ \gamma\ \leq1$

- 图表：

  ![1569657316464](C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\1569657316464.png)



## 三、联合熵与条件熵



### 1、联合熵

对X,Y的联合分布p(X,Y),其联合熵为


$$
H(X,Y)=-\sum \limits_{x,y} p(x,y)log\ p(x,y)=-E\ log\ p(x,y)
$$



### 2、条件熵

对X,Y的条件分布p(Y|X),其条件熵


$$
H(Y|X)=-\sum \limits_{x,y}p(x,y)log\ p(y|x)=-E\ log\ p(Y|X)\\
=\sum\limits_{x,y}p(x)H(Y|X=x)
$$



### 3、链式法则

- $H(X,Y)=H(X)+H(Y|X)$
- $H(X,Y)=H(Y)+H(X|Y)$





## 四、互信息



### 1、定义

- 数学定义

$$
I(X;Y)=\sum\limits_{x,y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}=E\ log\frac{p(X,Y)}{p(X)p(Y)}
$$

- 条件概率表示


$$
I(X;Y)=\sum\limits_{x,y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}\\
\ \ \ \ \ \ \ \ \ =\sum\limits_{x,y}p(x,y)log\frac{p(x|y)}{p(x)}\\
=E\ log\frac{p(X|Y)}{p(X)}
$$



### 2、与熵的关系

- $I(X;X)=H(X)$
- $I(X,Y)=H(X)-H(X|Y)$
- $I(X,Y)=H(Y)-H(Y|X)$
- $I(X,Y)=H(X)+H(Y)-H(X,Y)$



### 3、文氏图

![1569658934283](C:\Users\杨士伟\AppData\Roaming\Typora\typora-user-images\1569658934283.png)



### 4、条件互信息

$$
I(X;Y|Z)=\sum\limits_{x,y,z}p(x,y,z)log\frac{p(x,y|z)}{p(x|z)p(y|z)}\\=E\ log\frac{p(X,Y|Z)}{p(X|Z)p(Y|Z)}
$$



### 5、条件互信息与条件熵、联合熵的关系

- $I(X;X|Z)=H(X|Z)$
- $I(X;Y|Z)=H(X|Z)-H(X|Y,Z)$
- $I(X;Y|Z)=H(Y|Z)-H(Y|X,Z)$
- $I(X;Y|Z)=H(X|Z)+H(Y|Z)-H(X,Y|Z)$



## 五、链式法则



### 1、熵的链式法则

对于服从$p(x_1,x_2,...x_n)$的随机变量$x_1,x_2...x_n$
$$
H(X_1,X_2,...,X_n)=\sum\limits_{i=1}\limits^{n}H(X_i|X_1,...,X_{i-1})
$$



### 2、条件熵的链式法则

$$
H(X_1,X_2,...,X_n|Y)=\sum\limits_{i=1}\limits^{n}H(X_i|X_1,...,X_{i-1},Y)
$$



### 3、互信息的链式法则

$$
I(X_1,X_2,...,X_n;Y)=\sum\limits_{i=1}\limits^{n}I(X_i;Y|X_1,...,X_{i-1})
$$



## 六、相对熵



### 1、相对熵

在同一个样本空间下，不同概率分布p与q的相对熵：
$$
D(p||q)=\sum\limits_xp(x)log\frac{p(x)}{q(x)}\\=E_plog\frac{p(x)}{q(x)}
$$

- 相对熵体现不同分布间的距离
- 相对熵不对称



### 2、条件相对熵

$$
D(\ p(y|x)||\  q(y|x))=\sum\limits_x\sum\limits_yp(x,y)log\frac{p(y|x)}{q(y|x)}\\
=E_{p(x,y)}log\frac{p(y|x)}{q(y|x)}
$$



### 3、相对熵的链式法则

$D(p(y,x)||q(y,x))=D(p(x)||q(x))+D(p(y|x)||q(y|x))$



## 七、基础不等式



### 1、信息不等式

$D(p||q)\ge0$

- 当且仅当p=q时等号成立

**辅助推论1**

- 对于任意a>0，$lna\le a-1$
- 当且仅当a=1时等号成立

**辅助推论2**

- 对任意a>0,$lna\ge 1-\frac{1}{a}$
- 当且仅当a=1时等号成立



### 2、Jensen不等式

如果f(x)是凸函数，对于随机变量X:

$Ef(X)\ge f(EX)$



### 3、对数和不等式（log-sum)

对于非负数 $a_1,a_2,a_3,..,a_n\ \ \ \   b_1,b_2,b_3...b_n$

$\sum\limits_{i=1}^na_ilog\frac{a_i}{b_i}\ge (\sum\limits_{i=1}^na_i)log\frac{\sum\limits_{i=1}^na_i}{\sum\limits_{i=1}^nb_i}$

- 当且仅当$\frac{a_i}{b_i}=const$时等号成立

- 对数不等式和信息不等式互为充要条件



### 4、互信息非负性

对于随机变量X、Y,$I(X;Y)\ge0$,当且仅当X与Y相互独立时等号成立

**推论**

$I(X;Y|Z)\ge0$



### 5、条件减小熵

$H(X)\ge H(X|Y)\ge H(X|Y,Z)$

- 当且仅当X与Y相互独立时，等号成立
- 类似，$H(X|Y)\ge H(X|Y,Z)$



### 6、熵的独立边界

$H(X_1,X_2,X_3,...X_n)\le\sum\limits_{i=1}^nH(X_i)$

- 当且仅当X相互独立时，等号成立





### 7、熵的边界

当且仅当X为均匀分布时等号成立

$H(X)\le log|X|$

- 对于随机变量X，如果其字符集有限，则熵有限
- 对任意非负值，我们总可以找到一个随机变量X使其熵等于a
- 随机变量X的字符集如果无限，其熵可能有限也可能无限



### 8、数据处理不等式

$H(f(X))\le H(X)\\I(X;Y)\ge I(X;g(Y))$



## 八、互信息的凹性



### 1、定理1

相对熵$D(p||q)$是关于p,q的凸函数



### 2、定理2

熵$H(p)$是关于p的凹函数



### 3、推论

互信息$I(X;Y)$是关于p(x)的凹函数





## 九、数据处理不等式



### 1、马尔科夫链

如果随机变量XYZ的联合pmf可以写成

$p(x,y,z)=p(x)\ *p(y|x)\ *p(z|y)$

则称XYZ构成一个马尔科夫链，记作$X\rightarrow Y\rightarrow Z$



### 2、数据处理不等式

如果$X\rightarrow Y\rightarrow Z$构成了一个马尔科夫链，则有

$I(X;Z)\le I(X;Y)$

$I(X;Z)\le I(Z;Y)$

$H(X|Z)\ge H(X|Y)$

$I(X;Y|Z)\le I(X;Y)$

$I(X;Y,Z)\ge I(X;Y)$



**推论1**

如果$Z=g(Y)$,即Z是Y的函数，则$I(X;Y)\ge I(X;g(Y))$

**推论2**

如果$X\rightarrow Y\rightarrow Z$，则有$I(X;Y|Z)\le I(X;Y)$

**推论3**

$I(X；Y，Z)\ge I(X;Y)$

当且仅当$X\rightarrow Y\rightarrow Z$构成马尔科夫链时等号成立

**推论4**

如果$X\rightarrow Y\rightarrow Z$，则有$H(X|Z)\ge H(X|Y)$



## 十、Fano不等式



### 1、定义

$$
\begin{align}
&设X_1，X_2为在同一字符集X上取值的随机变量，则有：\\
           &H(X_1|X_2)\le h_b(P_e)+P_elog|X_1|\\
           &其中：\\
           &P_e=Pr\{X_1\ne X_2\}
\end{align}
$$



### 2、推论

$$
\begin{align}
&Fano不等式可以弱化为\\
&H(X_1|X_2)<1+P_elog(|X_1|)\\
&log(|X_1|)=nR
&
\end{align}
$$

**注意**

- 对有限字符集，当$P_e$趋向于0时，条件熵也趋向于0
- 对可数无穷大的字符集，未必成立



### 3、引理

$$
\begin{align}
&对于独立同分布的随机变量X,X^{'}，熵为H(X)，则有\\
&Pr\{X=X^{'}\}\ge2^{-H(X)}

\end{align}
$$

- 当且仅当X为均匀分布时等号成立

  



